{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xgmGUH_dXje2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Instalación de Dependencias\n",
        "!pip install mysql-connector-python"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpYOlU0XbIHR",
        "outputId": "cf27cf60-d852-4742-d62d-4a66ebcbfaba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-8.2.0-cp310-cp310-manylinux_2_17_x86_64.whl (31.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=4.21.12,>=4.21.1 (from mysql-connector-python)\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, mysql-connector-python\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mysql-connector-python-8.2.0 protobuf-4.21.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LG_VxW23SG0r"
      },
      "outputs": [],
      "source": [
        "# @title Import Libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import mysql.connector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se han elegido dos tablas para nuestra base de datos. Una con toda la información de los viajes y relacionados con estos y luego otra con los datos sobre las estaciones en base a su disponibilidad (Cantidad disponible, Maxima ocupación y demás)"
      ],
      "metadata": {
        "id": "u72dtiDbSvFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos de Viajes"
      ],
      "metadata": {
        "id": "pErK0v8WXfkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio de Queries  \n",
        "Aqui declaramos las queries para la creación de la tabla como la inserción de datos."
      ],
      "metadata": {
        "id": "xgmGUH_dXje2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Los datos que nos interesan son los siguientes: Fecha, Duración del viaje, Fecha Desbloqueo, Fecha Bloqueo,\n",
        "# @markdown ID Lugar Desbloqueo, Nombre Lugar de Desbloqueo, ID Lugar Bloqueo, Nombre Lugar de Bloqueo y Coordenadas de cada zona\n",
        "query_create_travels = \"CREATE TABLE travels (dates DATE, trip_minutes FLOAT, unlock_date TIMESTAMP,\" \\\n",
        "        \"lock_date TIMESTAMP, station_unlock VARCHAR(255), unlock_station_name VARCHAR(255), \" \\\n",
        "        \"station_lock VARCHAR(255), lock_station_name VARCHAR(255) , unlock_latitude FLOAT, unlock_longitude FLOAT, \" \\\n",
        "        \"lock_latitude FLOAT, lock_longitude FLOAT)\"\n",
        "\n",
        "query_insert_travels = \"INSERT INTO travels (dates, trip_minutes, unlock_date,\" \\\n",
        "        \"lock_date, station_unlock, unlock_station_name , \" \\\n",
        "        \"station_lock , lock_station_name , unlock_latitude, unlock_longitude, \" \\\n",
        "        \"lock_latitude, lock_longitude) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\""
      ],
      "metadata": {
        "id": "sQcL6nFDX1vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Iniciamos la conexion con la base de datos\n",
        "pd.set_option('display.max_colwidth', 255)\n",
        "\n",
        "host = \"\" # @param {type: \"string\"}\n",
        "user = \"\" # @param {type: \"string\"}\n",
        "password = \"\" # @param {type: \"string\"}\n",
        "database = \"\" # @param {type: \"string\"}\n",
        "\n",
        "mydb = mysql.connector.connect(\n",
        "  host=host,\n",
        "  user=user,\n",
        "  password=password,\n",
        "  database=database\n",
        ")\n",
        "\n",
        "mycursor = mydb.cursor()"
      ],
      "metadata": {
        "id": "8ynTooM5ajYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación Datos de Viajes  \n",
        "Antes de poder meter los datos en nuestra base de datos tenemos que analizar estos y ajustarlos a nuestras necesidades. No hay que hacer mucha limpieza de datos realmente, solo tener cuidado a la hora de leer los archivos ya que hay diferencias entre ficheros.  \n",
        "Con el siguiente codigo nos aseguramos que todos los archivos queden con los mismos datos y listos para insertar en la base de datos.\n"
      ],
      "metadata": {
        "id": "2CX2v2s-Thvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Limpieza de Datos e inserción en la base de datos\n",
        "# @markdown Para mayor eficiencia se leen los archivos csv de dentro de una carpeta y se iteran haciendo las mismas operaciones en ellos.\n",
        "# @markdown Por último se insertan los datos del csv en la tabla correspondiente en la base de datos.\n",
        "path_folder_files = \"Trips\" # @param {type: \"string\"}\n",
        "for file in os.listdir(path_folder_files):\n",
        "    df = pd.read_csv(f\"{path_folder_files}\\\\{file}\", sep=\";\")\n",
        "    df = df.drop(columns=[\"address_unlock\", \"address_lock\"])\n",
        "    df = df.dropna().reset_index().drop(columns=[\"index\"])\n",
        "    unlock_lat = []\n",
        "    unlock_lon = []\n",
        "    for d in df[\"geolocation_unlock\"]:\n",
        "        d = json.loads(d.replace(\"'\", \"\\\"\"))\n",
        "        unlock_lat.append(d[\"coordinates\"][1])\n",
        "        unlock_lon.append(d[\"coordinates\"][0])\n",
        "\n",
        "    lock_lat = []\n",
        "    lock_lon = []\n",
        "    for d in df[\"geolocation_lock\"]:\n",
        "        d = json.loads(d.replace(\"'\", \"\\\"\"))\n",
        "        lock_lat.append(d[\"coordinates\"][1])\n",
        "        lock_lon.append(d[\"coordinates\"][0])\n",
        "\n",
        "\n",
        "    df = df.drop(columns=[\"idBike\", \"geolocation_unlock\", \"geolocation_lock\", \"unlocktype\", \"locktype\", \"fleet\",\n",
        "                          \"dock_unlock\", \"dock_lock\"])\n",
        "    try:\n",
        "        df = df.drop(columns=[\"idTrip\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    df[\"unlock_latitude\"] = unlock_lat\n",
        "    df[\"unlock_longitude\"] = unlock_lon\n",
        "\n",
        "    df[\"lock_latitude\"] = lock_lat\n",
        "    df[\"lock_longitude\"] = lock_lon\n",
        "\n",
        "    df[\"unlock_date\"] = df[\"unlock_date\"].apply(lambda x: x.replace(\"T\", \" \"))\n",
        "    df[\"lock_date\"] = df[\"lock_date\"].apply(lambda x: x.replace(\"T\", \" \"))\n",
        "\n",
        "    df.to_csv(\"good_data.csv\", index=False)\n",
        "\n",
        "    df = pd.read_csv(\"good_data.csv\")\n",
        "\n",
        "    tuple_list = [tuple(x) for x in df.to_records(index=False)]\n",
        "\n",
        "    mycursor.executemany(query_insert_travels, tuple_list)\n",
        "    mydb.commit()\n",
        "    print(f\"File {file} inserted into the DB\")"
      ],
      "metadata": {
        "id": "NL0KLjTuVqA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se ha comentado no hay muchos detalles en la limpieza, si hay algo a destacar es formatear las fechas para que queden a corde a las variables de la base de datos y poco mas. Se han quitado algunas columnas que no aportaban información relevante o que no diese ya otra."
      ],
      "metadata": {
        "id": "2gR8lqDubroM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos de Estaciones"
      ],
      "metadata": {
        "id": "mH_P6hA8cQWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio de Queries  \n",
        "Aqui declaramos las queries para la creación de la tabla como la inserción de datos."
      ],
      "metadata": {
        "id": "FK3VW2SfcfcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_create_stations = \"CREATE TABLE stations (max_occupation INT, free_bases INT, longitude FLOAT, no_available INT,\" \\\n",
        "        \" address VARCHAR(255), latitude FLOAT, bikes_available INT, \" \\\n",
        "        \"id INT, date DATE, hours TIME)\"\n",
        "\n",
        "query_insert_statation = \"INSERT INTO stations (max_occupation, free_bases, longitude, no_available,\" \\\n",
        "            \" address, latitude, bikes_available, \" \\\n",
        "            \"id, date, hours) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\""
      ],
      "metadata": {
        "id": "vNzuGVUjcjy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación Datos sobre las Estaciones\n",
        "Lo mismo que se ha realizado antes, se procede a la limpieza de los datos para posteriormente añadirlos a la base de dato. En este caso se tratan de archivos JSON y se leen desde una carpeta al igual que antes.\n"
      ],
      "metadata": {
        "id": "se26a1Y4c5HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_folder_files = \"\" # @param {type: \"string\"}\n",
        "for file in os.listdir(path_folder_files):\n",
        "    df = pd.read_json(f\"{path_folder_files}\\\\{file}\", lines=True)\n",
        "    df[\"date\"] = df[\"_id\"].apply(lambda x: x.split(\"T\")[0])\n",
        "    df[\"hours\"] = df[\"_id\"].apply(lambda x: x.split(\"T\")[1].split(\".\")[0])\n",
        "    df = df.drop(columns=[\"_id\"])\n",
        "    true_df = pd.DataFrame()\n",
        "    for i in range(len(df)):\n",
        "        row = df[\"stations\"].iloc[i]\n",
        "        aux = pd.DataFrame(row)\n",
        "        aux[\"date\"] = df[\"date\"][i]\n",
        "        aux[\"hours\"] = df[\"hours\"][i]\n",
        "        true_df = pd.concat([true_df, aux])\n",
        "\n",
        "    true_df = true_df.drop(columns=[\"activate\", \"reservations_count\", \"light\", \"number\", \"name\"])\n",
        "    for i in true_df.columns:\n",
        "        try:\n",
        "            true_df[i] = true_df[i].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "    true_df.to_csv(\"stations_info.csv\", index=False)\n",
        "\n",
        "    tuple_list = [tuple(x) for x in true_df.to_records(index=False)]\n",
        "\n",
        "    mycursor.executemany(query_insert_statation, tuple_list)\n",
        "    mydb.commit()"
      ],
      "metadata": {
        "id": "3B9UbHpIc6Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte es verdad que hay más dificultad ya que se trata de un JSON pero en terminos de limpieza sigue sin haber muchos. Simplemente colocar los datos de manera que coincidad los de todos los archivos"
      ],
      "metadata": {
        "id": "vgkYjQXidwb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y con esto ya tendriamos la base de datos montada con las dos tablas de datos que queremos. Este notebook ha sido ejecutado en local, se recomienda hacer lo mismo montando la base de datos en una maquina propia."
      ],
      "metadata": {
        "id": "KTGhV4Qwr9lV"
      }
    }
  ]
}